import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pdfplumber
import re

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ç’°å¢ƒå¤‰æ•° DEBUG_MODE=true ã§æœ‰åŠ¹åŒ–
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'
DEBUG_LIMIT = 5  # ãƒ‡ãƒãƒƒã‚°æ™‚ã«å–å¾—ã™ã‚‹PDFæ•°

def get_all_pdf_urls_by_year(year):
    """æŒ‡å®šå¹´åº¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ã¦ã®stock_val PDFã®URLã‚’å–å¾—"""
    base_url = "https://www.jpx.co.jp"
    
    # å¹´åº¦ã«å¿œã˜ãŸã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã®URL
    year_to_archive = {
        2026: "00-00-archives-00.html",
        2025: "00-00-archives-01.html",
        2024: "00-00-archives-02.html",
        2023: "00-00-archives-03.html"
    }
    
    if year not in year_to_archive:
        print(f"è­¦å‘Š: {year}å¹´åº¦ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã¯å®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return []
    
    page_url = f"https://www.jpx.co.jp/markets/statistics-equities/investor-type/{year_to_archive[year]}"
    
    try:
        print(f"\n{year}å¹´åº¦ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {page_url}")
        response = requests.get(page_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # stock_val (é‡‘é¡ç‰ˆ) ã®PDFãƒªãƒ³ã‚¯ã‚’å…¨ã¦å–å¾—
        pdf_links = soup.find_all('a', href=lambda x: x and 'stock_val' in x and '.pdf' in x.lower())
        
        urls = []
        for link in pdf_links:
            href = link['href']
            
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            if href.startswith('/'):
                pdf_url = base_url + href
            elif href.startswith('http'):
                pdf_url = href
            else:
                pdf_url = base_url + '/' + href
            
            urls.append(pdf_url)
        
        print(f"  {year}å¹´åº¦: {len(urls)}ä»¶ã®PDFã‚’ç™ºè¦‹")
        return urls
        
    except Exception as e:
        print(f"{year}å¹´åº¦ã®URLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_latest_pdf_url():
    """JPXã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®é‡‘é¡ç‰ˆPDFãƒ•ã‚¡ã‚¤ãƒ«ã®URLã‚’å–å¾—"""
    base_url = "https://www.jpx.co.jp"
    page_url = "https://www.jpx.co.jp/markets/statistics-equities/investor-type/00-00-archives-00.html"
    
    try:
        response = requests.get(page_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã€Œæ ªå¼é€±é–“å£²è²·çŠ¶æ³ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        # stock_val (é‡‘é¡ç‰ˆ) ã®PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
        pdf_links = soup.find_all('a', href=lambda x: x and 'stock_val' in x and '.pdf' in x.lower())
        
        if not pdf_links:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã™ã¹ã¦ã®PDFãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¦é‡‘é¡ç‰ˆã‚’æ¢ã™
            all_pdf_links = soup.find_all('a', href=lambda x: x and '.pdf' in x.lower())
            pdf_links = [link for link in all_pdf_links if 'stock_val' in link.get('href', '')]
        
        if not pdf_links:
            raise ValueError("é‡‘é¡ç‰ˆPDF (stock_val) ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # æœ€åˆã®ãƒªãƒ³ã‚¯ã‚’æœ€æ–°ã¨ã¿ãªã™
        latest_link = pdf_links[0]['href']
        
        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
        if latest_link.startswith('/'):
            pdf_url = base_url + latest_link
        elif latest_link.startswith('http'):
            pdf_url = latest_link
        else:
            pdf_url = base_url + '/' + latest_link
            
        print(f"å–å¾—URL: {pdf_url}")
        
        # URLãŒæ­£ã—ãé‡‘é¡ç‰ˆ(stock_val)ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if 'stock_val' not in pdf_url:
            raise ValueError(f"é‡‘é¡ç‰ˆPDFã§ã¯ã‚ã‚Šã¾ã›ã‚“: {pdf_url}")
        
        return pdf_url
        
    except Exception as e:
        print(f"URLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def extract_date_from_filename(url):
    """
    PDFã®URLã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
    
    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ—¥ä»˜ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆä¾‹: stock_val_1_231204.pdf â†’ 2023-12-04ï¼‰
    2022å¹´ã®ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–ã™ã‚‹
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜éƒ¨åˆ†ã‚’æŠ½å‡º (ä¾‹: 231204)
        match = re.search(r'stock_val_\d+_(\d{6})\.pdf', url)
        if match:
            date_str = match.group(1)
            
            # YYMMDDã‚’è§£æ
            yy = int(date_str[0:2])
            month = int(date_str[2:4])
            day = int(date_str[4:6])
            
            # å¹´ã‚’åˆ¤å®š: 23ä»¥ä¸Šãªã‚‰2023å¹´ã€ãã‚Œä»¥ä¸‹ã¯é™¤å¤–
            if yy >= 23:
                year = 2000 + yy
            else:
                print(f"  è­¦å‘Š: 2023å¹´ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: 20{yy}-{month:02d}-{day:02d}")
                return None
            
            # YYYY-MM-DDå½¢å¼ã§è¿”ã™
            result_date = f"{year:04d}-{month:02d}-{day:02d}"
            print(f"  æŠ½å‡ºæ—¥ä»˜: {result_date}")
            
            return result_date
            
    except Exception as e:
        print(f"æ—¥ä»˜æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    return None

def download_pdf(url, filename='temp.pdf'):
    """PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        with open(filename, 'wb') as f:
            f.write(response.content)
        
        print(f"PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {filename}")
        return filename
    except Exception as e:
        print(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def extract_from_pdf(pdf_path):
    """PDFã‹ã‚‰æµ·å¤–æŠ•è³‡å®¶ã®å·®å¼•ãé‡‘é¡ã‚’æŠ½å‡ºï¼ˆè²·ã„é‡‘é¡ - å£²ã‚Šé‡‘é¡ã§è¨ˆç®—ï¼‰"""
    
    try:
        print("PDFã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡ºä¸­...")
        
        with pdfplumber.open(pdf_path) as pdf:
            page = pdf.pages[0]
            print(f"\n=== ãƒšãƒ¼ã‚¸ 1 ã‚’å‡¦ç† ===")
            
            tables = page.extract_tables()
            
            if not tables:
                raise ValueError("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            for table_num, table in enumerate(tables):
                for row_idx, row in enumerate(table):
                    if not row:
                        continue
                    
                    row_text = ' '.join([str(cell) if cell else '' for cell in row])
                    
                    # æµ·å¤–æŠ•è³‡å®¶ã®å£²ã‚Šè¡Œ
                    if ('æµ·å¤–æŠ•è³‡å®¶' in row_text or 'Foreigners' in row_text) and ('å£²ã‚Š' in row_text or 'Sales' in row_text):
                        print(f"\næµ·å¤–æŠ•è³‡å®¶(å£²ã‚Š)è¡Œã‚’ç™ºè¦‹:")
                        print(f"  è¡Œãƒ‡ãƒ¼ã‚¿: {row}")
                        
                        # å£²ã‚Šé‡‘é¡ã‚’å–å¾—ï¼ˆæœ€åˆã®å¤§ããªæ•°å€¤ï¼‰
                        sell_amount = None
                        for cell in row:
                            if cell and str(cell).strip():
                                # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å¤§ããªæ•°å€¤ã‚’æ¢ã™
                                match = re.search(r'\d{1,3}(?:,\d{3})+', str(cell))
                                if match:
                                    try:
                                        value = int(match.group().replace(',', ''))
                                        # 100å„„ä»¥ä¸Šã®æ•°å€¤ï¼ˆå£²è²·é‡‘é¡ãƒ¬ãƒ™ãƒ«ï¼‰
                                        if value >= 1000000000:
                                            sell_amount = value
                                            print(f"  å£²ã‚Šé‡‘é¡: {sell_amount:,}")
                                            break
                                    except ValueError:
                                        continue
                        
                        if sell_amount is None:
                            print("  è­¦å‘Š: å£²ã‚Šé‡‘é¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                            continue
                        
                        # æ¬¡ã®è¡Œã‚’è²·ã„è¡Œã¨ã—ã¦å‡¦ç†
                        buy_amount = None
                        if row_idx + 1 < len(table):
                            next_row = table[row_idx + 1]
                            next_row_text = ' '.join([str(cell) if cell else '' for cell in next_row])
                            
                            print(f"\næ¬¡ã®è¡Œï¼ˆè²·ã„è¡Œã¨æ¨å®šï¼‰:")
                            print(f"  è¡Œãƒ‡ãƒ¼ã‚¿: {next_row}")
                            
                            if 'è²·ã„' in next_row_text or 'Purchases' in next_row_text or 'Foreigners' in next_row_text:
                                # è²·ã„é‡‘é¡ã‚’å–å¾—ï¼ˆæœ€åˆã®å¤§ããªæ•°å€¤ï¼‰
                                for cell in next_row:
                                    if cell and str(cell).strip():
                                        # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å¤§ããªæ•°å€¤ã‚’æ¢ã™
                                        match = re.search(r'\d{1,3}(?:,\d{3})+', str(cell))
                                        if match:
                                            try:
                                                value = int(match.group().replace(',', ''))
                                                # 100å„„ä»¥ä¸Šã®æ•°å€¤ï¼ˆå£²è²·é‡‘é¡ãƒ¬ãƒ™ãƒ«ï¼‰
                                                if value >= 1000000000:
                                                    buy_amount = value
                                                    print(f"  è²·ã„é‡‘é¡: {buy_amount:,}")
                                                    break
                                            except ValueError:
                                                continue
                        
                        if buy_amount is None:
                            print("  è­¦å‘Š: è²·ã„é‡‘é¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                            continue
                        
                        # å·®å¼•ãã‚’è¨ˆç®—: è²·ã„ - å£²ã‚Š
                        balance = buy_amount - sell_amount
                        
                        if balance > 0:
                            print(f"\nâœ“ è²·ã„è¶…: {balance:,} (è²·ã„ {buy_amount:,} - å£²ã‚Š {sell_amount:,})")
                        elif balance < 0:
                            print(f"\nâœ“ å£²ã‚Šè¶…: {balance:,} (è²·ã„ {buy_amount:,} - å£²ã‚Š {sell_amount:,})")
                        else:
                            print(f"\nâœ“ å‡è¡¡: 0")
                        
                        return balance
        
        raise ValueError("æµ·å¤–æŠ•è³‡å®¶è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        print(f"PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        raise

def save_to_csv(value, date_str=None):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    csv_file = 'history.csv'
    
    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')
    
    # æ—¢å­˜ã®CSVã‚’èª­ã¿è¾¼ã‚€ã‹ã€æ–°è¦ä½œæˆ
    if os.path.exists(csv_file):
        df = pd.read_csv(csv_file)
    else:
        df = pd.DataFrame(columns=['date', 'balance'])
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    new_row = pd.DataFrame({'date': [date_str], 'balance': [value]})
    df = pd.concat([df, new_row], ignore_index=True)
    
    # é‡è¤‡å‰Šé™¤ï¼ˆåŒã˜æ—¥ä»˜ã®å ´åˆã¯æœ€æ–°ã‚’ä¿æŒï¼‰
    df = df.drop_duplicates(subset=['date'], keep='last')
    
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date')
    df['date'] = df['date'].dt.strftime('%Y-%m-%d')
    
    df.to_csv(csv_file, index=False)
    print(f"CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {date_str} - {value:,}")

def create_trend_chart():
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
    csv_file = 'history.csv'
    
    if not os.path.exists(csv_file):
        print("CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã§ãã¾ã›ã‚“")
        return
    
    df = pd.read_csv(csv_file)
    
    if len(df) == 0:
        print("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã§ãã¾ã›ã‚“")
        return
    
    # æ—¥ä»˜ã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
    df['date'] = df['date'].astype(str)
    
    # ã‚°ãƒ©ãƒ•ä½œæˆ
    plt.figure(figsize=(12, 6))
    plt.plot(df['date'], df['balance'], marker='o', linewidth=2, markersize=8)
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Balance (JPY)', fontsize=12)
    plt.title('Foreign Investors Balance Trend', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig('trend.png', dpi=150)
    print("ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: trend.png")

def process_historical_data():
    """éå»ãƒ‡ãƒ¼ã‚¿ï¼ˆ2023-2026å¹´åº¦ï¼‰ã‚’å…¨ã¦å–å¾—ã—ã¦ä¿å­˜"""
    
    if DEBUG_MODE:
        print("\n" + "="*60)
        print("âš ï¸  DEBUG MODE ACTIVE - 2023å¹´ãƒšãƒ¼ã‚¸ã®ä¸Šã‹ã‚‰5ä»¶ã®ã¿å–å¾—")
        print("="*60 + "\n")
    else:
        print("\n=== éå»ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹ ===")
    
    # æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
    csv_file = 'history.csv'
    if os.path.exists(csv_file):
        os.remove(csv_file)
        print("æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
    
    if DEBUG_MODE:
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: 2023å¹´ãƒšãƒ¼ã‚¸ã®ä¸Šã‹ã‚‰5ä»¶ã®ã¿å–å¾—ï¼ˆã‚½ãƒ¼ãƒˆãªã—ï¼‰
        urls_2023 = get_all_pdf_urls_by_year(2023)
        all_urls = urls_2023[:DEBUG_LIMIT]  # ä¸Šã‹ã‚‰5ä»¶
        print(f"ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: 2023å¹´ãƒšãƒ¼ã‚¸ã®ä¸Šã‹ã‚‰ {len(all_urls)} ä»¶ã®PDFã‚’å‡¦ç†ã—ã¾ã™")
        print(f"URLä¾‹: {all_urls[0] if all_urls else 'ãªã—'}")
    else:
        # æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰: å…¨å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        all_urls = []
        for year in [2023, 2024, 2025, 2026]:
            urls = get_all_pdf_urls_by_year(year)
            all_urls.extend(urls)
        print(f"\nåˆè¨ˆ {len(all_urls)} ä»¶ã®PDFã‚’å‡¦ç†ã—ã¾ã™")
    
    success_count = 0
    error_count = 0
    skip_count = 0
    
    for idx, url in enumerate(all_urls, 1):
        try:
            print(f"\n[{idx}/{len(all_urls)}] å‡¦ç†ä¸­: {url}")
            
            # æ—¥ä»˜ã‚’æŠ½å‡º
            date_str = extract_date_from_filename(url)
            if not date_str:
                print(f"  ã‚¹ã‚­ãƒƒãƒ—: æ—¥ä»˜ãŒç„¡åŠ¹ã¾ãŸã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¾ã—ãŸ")
                skip_count += 1
                continue
            
            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            pdf_path = download_pdf(url, f'temp_{idx}.pdf')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            balance = extract_from_pdf(pdf_path)
            
            # CSVã«ä¿å­˜
            save_to_csv(balance, date_str)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(pdf_path):
                os.remove(pdf_path)
            
            success_count += 1
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            error_count += 1
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            temp_file = f'temp_{idx}.pdf'
            if os.path.exists(temp_file):
                os.remove(temp_file)
            continue
    
    print(f"\n=== éå»ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
    print(f"æˆåŠŸ: {success_count}ä»¶, ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶, ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")

def main():
    pdf_path = None
    try:
        if DEBUG_MODE:
            print("\n" + "ğŸ› "*20)
            print("   DEBUG MODE: é«˜é€Ÿãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")
            print("   2023å¹´ãƒšãƒ¼ã‚¸ã®ä¸Šã‹ã‚‰5ä»¶ã®PDFã®ã¿å‡¦ç†ã—ã¾ã™")
            print("ğŸ› "*20 + "\n")
        
        csv_file = 'history.csv'
        
        # åˆå›å®Ÿè¡Œåˆ¤å®š: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆ
        should_get_historical = False
        
        if not os.path.exists(csv_file):
            should_get_historical = True
            print("=== åˆå›å®Ÿè¡Œ: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ ===\n")
        else:
            try:
                df = pd.read_csv(csv_file)
                if len(df) < 10:  # ãƒ‡ãƒ¼ã‚¿ãŒ10ä»¶æœªæº€ãªã‚‰éå»ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—
                    should_get_historical = True
                    print("=== ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ ===\n")
                else:
                    print("=== é€šå¸¸å®Ÿè¡Œ: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—ã—ã¾ã™ ===\n")
            except (pd.errors.EmptyDataError, pd.errors.ParserError):
                should_get_historical = True
                print("=== CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹ãŸã‚ã€å†å–å¾—ã—ã¾ã™ ===\n")
        
        if should_get_historical:
            process_historical_data()
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        print("\n=== æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å–å¾— ===")
        
        # æœ€æ–°ã®PDF URLã‚’å–å¾—
        pdf_url = get_latest_pdf_url()
        
        # æ—¥ä»˜ã‚’æŠ½å‡º
        date_str = extract_date_from_filename(pdf_url)
        
        if not date_str:
            print("è­¦å‘Š: æ—¥ä»˜ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        pdf_path = download_pdf(pdf_url)
        
        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        balance = extract_from_pdf(pdf_path)
        
        # CSVä¿å­˜
        save_to_csv(balance, date_str)
        
        # ã‚°ãƒ©ãƒ•ä½œæˆ
        create_trend_chart()
        
        print("\n=== å‡¦ç†å®Œäº† ===")
        
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if pdf_path and os.path.exists(pdf_path):
            os.remove(pdf_path)
            print("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
